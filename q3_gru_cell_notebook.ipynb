{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. W = 0, U = 1 and b=0. Assuming, each numeric value is input at time 't', These weights ensure that no matter what the history may be, input value is always considered. But each input is a sequence. This doesn't answer the question.\n",
    "\n",
    "Input: [[0],\n",
    "       [1],\n",
    "       [0,0],\n",
    "       [1,0],\n",
    "       [0,0,0],\n",
    "       [1,0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is: U=1; W=1 and b=0. The intent is to remember the state of the history from the first input and not change it later. This RNN weight scheme tells that equal importance to be given to input and history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GRU weight scheme to remeber history from the first input and not reset it with the subsequent inputs: U_z = 0; W_z = 1 and U_h = 1, given W_r and Z_r -0 and all biases to be 0. Ti doesn't matter what value W_h is because, it it gets multiplied by the all zero r_t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.. Switching state when input = 1: The reason it may not be modeled with 1-d input is that h_t depends both on x_t and h_t-1.\n",
    "This inidcates that same importance must be assigned to the input and hidden state. However, this alwyas creates either positive o negative input irrespective of the bias value, previous state and current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"hw3.q3.1\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"Wrapper around our GRU cell implementation that allows us to play\n",
    "    nicely with TensorFlow.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, state_size):\n",
    "        self.input_size = input_size\n",
    "        self._state_size = state_size\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Updates the state using the previous @state and @inputs.\n",
    "        Remember the GRU equations are:\n",
    "\n",
    "        z_t = sigmoid(x_t U_z + h_{t-1} W_z + b_z)\n",
    "        r_t = sigmoid(x_t U_r + h_{t-1} W_r + b_r)\n",
    "        o_t = tanh(x_t U_o + r_t * h_{t-1} W_o + b_o)\n",
    "        h_t = z_t * h_{t-1} + (1 - z_t) * o_t\n",
    "\n",
    "        TODO: In the code below, implement an GRU cell using @inputs\n",
    "        (x_t above) and the state (h_{t-1} above).\n",
    "            - Define W_r, U_r, b_r, W_z, U_z, b_z and W_o, U_o, b_o to\n",
    "              be variables of the apporiate shape using the\n",
    "              `tf.get_variable' functions.\n",
    "            - Compute z, r, o and @new_state (h_t) defined above\n",
    "        Tips:\n",
    "            - Remember to initialize your matrices using the xavier\n",
    "              initialization as before.\n",
    "        Args:\n",
    "            inputs: is the input vector of size [None, self.input_size]\n",
    "            state: is the previous state vector of size [None, self.state_size]\n",
    "            scope: is the name of the scope to be used when defining the variables inside.\n",
    "        Returns:\n",
    "            a pair of the output vector and the new state vector.\n",
    "        \"\"\"\n",
    "        scope = scope or type(self).__name__\n",
    "\n",
    "        # It's always a good idea to scope variables in functions lest they\n",
    "        # be defined elsewhere!\n",
    "        with tf.variable_scope(scope):\n",
    "            ### YOUR CODE HERE (~20-30 lines)\n",
    "            # Update gate\n",
    "            U_z = tf.get_variable(name=\"U_z\",shape=(self.input_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W_z = tf.get_variable(name=\"W_z\",shape=(self._state_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_z = tf.get_variable(name=\"b_z\",shape=(self._state_size,),dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "            \n",
    "            # Reset gate\n",
    "            U_r = tf.get_variable(name=\"U_r\",shape=(self.input_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W_r = tf.get_variable(name=\"W_r\",shape=(self._state_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_r = tf.get_variable(name=\"b_r\",shape=(self._state_size,),dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "            \n",
    "            # Intermediate hidden state\n",
    "            U_o = tf.get_variable(name=\"U_o\",shape=(self.input_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W_o = tf.get_variable(name=\"W_o\",shape=(self._state_size,self._state_size),dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b_o = tf.get_variable(name=\"b_o\",shape=(self._state_size,),dtype=tf.float32,initializer=tf.initializers.zeros())\n",
    "            \n",
    "            # Compute update_gate value\n",
    "            z_t = tf.nn.sigmoid(tf.matmul(inputs,U_z) + tf.matmul(state,W_z) + b_z)\n",
    "            \n",
    "            # Compute reset_gate value\n",
    "            r_t = tf.nn.sigmoid(tf.matmul(inputs,U_r) + tf.matmul(state,W_r) + b_r)\n",
    "            \n",
    "            # Compute intermediate hidden state value\n",
    "            o_t = tf.nn.tanh(tf.matmul(inputs,U_r) + tf.math.multiply(r_t,tf.matmul(state,W_r))  + b_r)\n",
    "            \n",
    "            # Final hidden state computation\n",
    "            update_gate_shape = tf.shape(z_t)\n",
    "            one_m_z_t = tf.ones(shape = (update_gate_shape[0],self._state_size),name=\"1_m_z_t\") - z_t\n",
    "            new_state = tf.math.multiply(z_t,state) + tf.math.multiply(one_m_z_t, o_t)\n",
    "            \n",
    "            ### END YOUR CODE ###\n",
    "        # For a GRU, the output and state are the same (N.B. this isn't true\n",
    "        # for an LSTM, though we aren't using one of those in our\n",
    "        # assignment)\n",
    "        output = new_state\n",
    "        return output, new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gru_cell():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.variable_scope(\"test_gru_cell\"):\n",
    "            x_placeholder = tf.placeholder(tf.float32, shape=(None,3))\n",
    "            h_placeholder = tf.placeholder(tf.float32, shape=(None,2))\n",
    "\n",
    "            with tf.variable_scope(\"gru\"):\n",
    "                tf.get_variable(\"U_r\", initializer=np.array(np.eye(3,2), dtype=np.float32))\n",
    "                tf.get_variable(\"W_r\", initializer=np.array(np.eye(2,2), dtype=np.float32))\n",
    "                tf.get_variable(\"b_r\",  initializer=np.array(np.ones(2), dtype=np.float32))\n",
    "                tf.get_variable(\"U_z\", initializer=np.array(np.eye(3,2), dtype=np.float32))\n",
    "                tf.get_variable(\"W_z\", initializer=np.array(np.eye(2,2), dtype=np.float32))\n",
    "                tf.get_variable(\"b_z\",  initializer=np.array(np.ones(2), dtype=np.float32))\n",
    "                tf.get_variable(\"U_o\", initializer=np.array(np.eye(3,2), dtype=np.float32))\n",
    "                tf.get_variable(\"W_o\", initializer=np.array(np.eye(2,2), dtype=np.float32))\n",
    "                tf.get_variable(\"b_o\",  initializer=np.array(np.ones(2), dtype=np.float32))\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            cell = GRUCell(3, 2)\n",
    "            y_var, ht_var = cell(x_placeholder, h_placeholder, scope=\"gru\")\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as session:\n",
    "                session.run(init)\n",
    "                x = np.array([\n",
    "                    [0.4, 0.5, 0.6],\n",
    "                    [0.3, -0.2, -0.1]], dtype=np.float32)\n",
    "                h = np.array([\n",
    "                    [0.2, 0.5],\n",
    "                    [-0.3, -0.3]], dtype=np.float32)\n",
    "                y = np.array([\n",
    "                    [ 0.320, 0.555],\n",
    "                    [-0.006, 0.020]], dtype=np.float32)\n",
    "                ht = y\n",
    "\n",
    "                y_, ht_ = session.run([y_var, ht_var], feed_dict={x_placeholder: x, h_placeholder: h})\n",
    "                print(\"y_ = \" + str(y_))\n",
    "                print(\"ht_ = \" + str(ht_))\n",
    "\n",
    "                assert np.allclose(y_, ht_), \"output and state should be equal.\"\n",
    "                assert np.allclose(ht, ht_, atol=1e-2), \"new state vector does not seem to be correct.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_test(_):\n",
    "    logger.info(\"Testing gru_cell\")\n",
    "    test_gru_cell()\n",
    "    logger.info(\"Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Testing gru_cell\n",
      "INFO:Passed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_ = [[ 0.32035077  0.55478156]\n",
      " [-0.00592546  0.0195577 ]]\n",
      "ht_ = [[ 0.32035077  0.55478156]\n",
      " [-0.00592546  0.0195577 ]]\n"
     ]
    }
   ],
   "source": [
    "do_test(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
